{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "1. How to start Spark Colab.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMweZASJjdmuMlesAL8bCF4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yejoonlee/TSA_Projects/blob/main/Spark/1_How_to_start_Spark_Colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Spark 환경설정 for Mac**\n",
        "\n",
        "[ 필요 사항 ]  \n",
        "*  Python (Anaconda ghkfdyd)\n",
        "*  Jupyter notebook\n",
        "*  brew (brew.sh)\n",
        "*  Java (brew tap AdoptOpenJDK/openjdk -> brew install --cask adoptopenjdk8)\n",
        "*  Scala (brew install scala)\n",
        "*  Spark (brew install apache-spark)\n",
        "*  Pyspark (pip install pyspark)  \n",
        "  \n",
        "\n",
        "--------------------------\n",
        "\n",
        "\n",
        "위 과정을 통해 환경을 설정한 후 local machine에서 pyspark 명령어를 통해 spark terminal을 실행시켰으나 Py4JJavaError만 일어나고 제대로 실행되지 않았다.  \n",
        "\n",
        "\n",
        "처음부터 Colab을 활용하지 않았던 이유는 spark를 쓰려면 환경 접근에 대한 자유도가 중요하지 않을까 싶었던 점과 terminal에 접근할 필요성이 있다는 것 때문이었다.  \n",
        "\n",
        "\n",
        "어쩔 수 없이 Colab으로 와서 작업을 해보는데 전과 다르게 terminal에 입력도 할 수 있게 되어서(*전에도 됐는데 몰랐던 건가*) 앞으로는 Colab으로 작업해볼 수 있을 것 같다(*사실 local 업그레이드 전까지는 방법도 없다*)."
      ],
      "metadata": {
        "id": "VazEx8qncrZH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Spark 환경설정 for Colab**\n",
        "\n",
        "Colab에서 Spark를 실습 및 작업해보기는 오히려 간단하다.  \n",
        "위와 같이 이것저것 깔지 않고 pip으로 pyspark만 깔아도 작업을 수행할 준비가 완료된다."
      ],
      "metadata": {
        "id": "_ooCBweorwN7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "UDqgTXt6cjtY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "890edcee-a8bb-445b-f81f-7d7f21044818"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.2.0.tar.gz (281.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 281.3 MB 37 kB/s \n",
            "\u001b[?25hCollecting py4j==0.10.9.2\n",
            "  Downloading py4j-0.10.9.2-py2.py3-none-any.whl (198 kB)\n",
            "\u001b[K     |████████████████████████████████| 198 kB 54.3 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.2.0-py2.py3-none-any.whl size=281805912 sha256=88dc377a98ef306a37fae59eb9bbcc326408f8a57a41cbe9afa3bd2ac9ebfd86\n",
            "  Stored in directory: /root/.cache/pip/wheels/0b/de/d2/9be5d59d7331c6c2a7c1b6d1a4f463ce107332b1ecd4e80718\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "Successfully installed py4j-0.10.9.2 pyspark-3.2.0\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspark"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.appName(\"taxi-analysis\").getOrCreate()"
      ],
      "metadata": {
        "id": "0ADmnBi1omeF"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pyspark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "maEBHhnzpRaB",
        "outputId": "ce417755-e3bb-4a90-a920-5e9f660ae1df"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python 3.7.12 (default, Sep 10 2021, 00:21:48) \n",
            "[GCC 7.5.0] on linux\n",
            "Type \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n",
            "WARNING: An illegal reflective access operation has occurred\n",
            "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/usr/local/lib/python3.7/dist-packages/pyspark/jars/spark-unsafe_2.12-3.2.0.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
            "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
            "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
            "WARNING: All illegal access operations will be denied in a future release\n",
            "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
            "Setting default log level to \"WARN\".\n",
            "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
            "21/12/30 14:02:28 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
            "21/12/30 14:02:29 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
            "Welcome to\n",
            "      ____              __\n",
            "     / __/__  ___ _____/ /__\n",
            "    _\\ \\/ _ \\/ _ `/ __/  '_/\n",
            "   /__ / .__/\\_,_/_/ /_/\\_\\   version 3.2.0\n",
            "      /_/\n",
            "\n",
            "Using Python version 3.7.12 (default, Sep 10 2021 00:21:48)\n",
            "Spark context Web UI available at http://80a3eafd5000:4041\n",
            "Spark context available as 'sc' (master = local[*], app id = local-1640872949916).\n",
            "SparkSession available as 'spark'.\n",
            ">>> exit()\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Ok0frlC7piMQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}